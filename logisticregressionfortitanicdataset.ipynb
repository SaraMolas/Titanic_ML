{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T17:32:00.894546Z","iopub.execute_input":"2024-05-01T17:32:00.895673Z","iopub.status.idle":"2024-05-01T17:32:02.315879Z","shell.execute_reply.started":"2024-05-01T17:32:00.895621Z","shell.execute_reply":"2024-05-01T17:32:02.314390Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**STEP 1: IMPORT PACKAGES AND LOAD DATA**\n\nThe first step is to import the packages that will be used to manipulate and explore the data, as well as the desired machine learning model to test. \nWe also need to load the training and test datasets.","metadata":{}},{"cell_type":"code","source":"# 1.1. Import required packages that will be used later\n\n# Import several Python packages used for ploting figures\nimport seaborn as sns # to make figures\nfrom IPython.display import display # to display multiple seaborn figures generated in the same notebook cell\nimport matplotlib.pyplot as plt # to make some plots and display multiple seaborn figures generated in the same notebook cell\n\n# Import packages to model data from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression # Logistic regression package\nfrom sklearn.ensemble import RandomForestClassifier # Random forest classifier package\nfrom sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler # to scale variables","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:32:41.096038Z","iopub.execute_input":"2024-05-01T17:32:41.096761Z","iopub.status.idle":"2024-05-01T17:32:43.192136Z","shell.execute_reply.started":"2024-05-01T17:32:41.096721Z","shell.execute_reply":"2024-05-01T17:32:43.190696Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 1.2. Import the Titanic datasets\n# When working with data, typically the dataset needs to be split into a training dataset to fit the model and a test dataset to validate the model.\n# In this case, Kaggle has already split the data, so we will import the training and test dataset separately\n\ntrainSet = pd.read_csv('/kaggle/input/titanic/train.csv') # Import training set\ntestSet = pd.read_csv('/kaggle/input/titanic/test.csv') # Import test set\nprint('Loaded datasets') # display a message to confirm that the data has been loaded","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:32:47.285830Z","iopub.execute_input":"2024-05-01T17:32:47.286320Z","iopub.status.idle":"2024-05-01T17:32:47.327801Z","shell.execute_reply.started":"2024-05-01T17:32:47.286282Z","shell.execute_reply":"2024-05-01T17:32:47.326547Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loaded datasets\n","output_type":"stream"}]},{"cell_type":"code","source":"# 1.3. Obtain basic information about the training data \nprint(trainSet.head()) # get the names of the columns of the dataset, that will constitute the variables or features of our model, and the first 5 rows of data\nprint('training data shape:', trainSet.shape) # get the shape(rows,columns) of the training dataset. rows = samples, columns = variables","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:24:53.576411Z","iopub.execute_input":"2024-04-25T14:24:53.576751Z","iopub.status.idle":"2024-04-25T14:24:53.588412Z","shell.execute_reply.started":"2024-04-25T14:24:53.576722Z","shell.execute_reply":"2024-04-25T14:24:53.586800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**STEP 2: EXPLORE THE DATA**\n\nBefore training a Machine Learning model, it is useful to explore the data, visualizing the different variables against the output. This will help us understand which variables are more important, in other words, are better predictors of the output. This information might be used later to improve our model. ","metadata":{}},{"cell_type":"code","source":"# 2. Let's explore the data to get an idea of the relevance of each variable: plots of each variable(or features) against output (= survival)\n# ignore full name, ticket number and cabin number for now\n\nsns.barplot(trainSet, x = 'Pclass', y = 'Survived')\nplt.show()\nsns.barplot(trainSet, x = 'Sex', y = 'Survived')\nplt.show()\nsns.violinplot(trainSet, x = 'Survived', y = 'Age')\nplt.show()\nsns.barplot(trainSet, x = 'SibSp', y = 'Survived')\nplt.show()\nsns.barplot(trainSet, x = 'Parch', y = 'Survived')\nplt.show()\nsns.violinplot(trainSet, x = 'Survived', y = 'Fare')\nplt.show()\nsns.barplot(trainSet, x = 'Embarked', y = 'Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:24:55.770062Z","iopub.execute_input":"2024-04-25T14:24:55.770411Z","iopub.status.idle":"2024-04-25T14:24:57.164844Z","shell.execute_reply.started":"2024-04-25T14:24:55.770381Z","shell.execute_reply":"2024-04-25T14:24:57.163207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These plots suggest that women and passengers in the first class (higher socioeconomic status) were more likely to survive (0 = did not survive, 1 = survived). ","metadata":{}},{"cell_type":"markdown","source":"**STEP 3: DATA ENGINEERING**\n\nThere might be some variables that contain missing values (NaN) in some entries. If that is the case, since ML models do not accept missing values we either have to drop those samples, which will reduce the amount of data available for training and increase noise, or we can replace those missing values by an estimate of the most likely value based on other variables. \n\nIn addition, as displayed in Step 1, some variables in the dataset are categorical. However, logistic regression only accepts numerical variables. Thus, we need to convert all categorical variables in the dataset into numerical ones. \n\nWe need to do the same modifications in both the training and test datasets. ","metadata":{}},{"cell_type":"code","source":"# 3.1. Let's see if there is any column in the dataframe with NaN values\n\nfor column in trainSet: # Iterate over each column in the dataset\n    if trainSet[column].isnull().any(): # check if the column contains any missing values\n        print('column with nans: ',column) # if the column contains missing values, print the column name","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:24:57.590985Z","iopub.execute_input":"2024-04-25T14:24:57.591347Z","iopub.status.idle":"2024-04-25T14:24:57.599779Z","shell.execute_reply.started":"2024-04-25T14:24:57.591322Z","shell.execute_reply":"2024-04-25T14:24:57.598355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the output from the above cell, the variables Age, Cabin and Embarked contain NaNs. We are going to use the Age and Embarked variable for our model, but we are not going to use the Cabin number. Thus, we only need to replace the missing values in the Age and Embarked columns. ","metadata":{}},{"cell_type":"code","source":"# 3.2.1. We could just replace the missing values by the median age of all passengers. However, just in case, let's investigate \n# if there are differences in passenger's age depending on gender and socioeconomic status \n\nsns.violinplot(trainSet, x = 'Sex', y = 'Age', hue='Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:25:03.027048Z","iopub.execute_input":"2024-04-25T14:25:03.027374Z","iopub.status.idle":"2024-04-25T14:25:03.312452Z","shell.execute_reply.started":"2024-04-25T14:25:03.027353Z","shell.execute_reply":"2024-04-25T14:25:03.311664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown in this plot, males tended to be older than females. In addition, people from class 1 tended to be older than passengers in classes 2 and 3. Thus, we should replace the missing values in the 'Age' variable by the median of the passenger's sex & class group.","metadata":{}},{"cell_type":"code","source":"# 3.2.2. Replace missing values in Age column with the median of the passengers sex and class group\n\ntrainSet['Age'] = trainSet['Age'].fillna(trainSet.groupby(['Pclass', 'Sex'])['Age'].transform('median'))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:25:05.827443Z","iopub.execute_input":"2024-04-25T14:25:05.827829Z","iopub.status.idle":"2024-04-25T14:25:05.840461Z","shell.execute_reply.started":"2024-04-25T14:25:05.827805Z","shell.execute_reply":"2024-04-25T14:25:05.838922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3.3.1. Some values in the Embarkment variable are missing too, let's check if the Embarkment site varied depending on \n# socioeconomic status (for instance richer passengers might leave in areas different to working class passengers)\n\nsns.countplot(trainSet, x = 'Embarked', hue='Pclass')\n#a.set_yticks(range(5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:30:41.879733Z","iopub.execute_input":"2024-04-25T14:30:41.880093Z","iopub.status.idle":"2024-04-25T14:30:42.107736Z","shell.execute_reply.started":"2024-04-25T14:30:41.880068Z","shell.execute_reply":"2024-04-25T14:30:42.106269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For all passenger classes, the most common embarkment site is 'S'.","metadata":{}},{"cell_type":"code","source":"# 3.3.2. We'll replace the Embarked missing values by the most common embarkment site of each passenger class\n\ntrainSet['Embarked'] = trainSet['Embarked'].fillna(trainSet['Embarked'].mode().iloc[0]) ","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:32:55.957744Z","iopub.execute_input":"2024-04-25T14:32:55.958095Z","iopub.status.idle":"2024-04-25T14:32:55.963686Z","shell.execute_reply.started":"2024-04-25T14:32:55.958072Z","shell.execute_reply":"2024-04-25T14:32:55.962319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3.4. Let's see if there is any column in the dataframe with NaN values\n\nfor column in testSet: # Iterate over each column in the dataset\n    if testSet[column].isnull().any(): # check if the column contains any missing values\n        print('column with nans: ',column) # if the column contains missing values, print the column name","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:32:57.679966Z","iopub.execute_input":"2024-05-01T17:32:57.681026Z","iopub.status.idle":"2024-05-01T17:32:57.696809Z","shell.execute_reply.started":"2024-05-01T17:32:57.680976Z","shell.execute_reply":"2024-05-01T17:32:57.695228Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"column with nans:  Age\ncolumn with nans:  Fare\ncolumn with nans:  Cabin\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the test dataset, Age, Fare and Cabin variables contains missing values. ","metadata":{}},{"cell_type":"code","source":"# 3.5.1. Replace missing values in Age column with the median of the passengers sex and class group\n\ntestSet['Age'] = testSet['Age'].fillna(testSet.groupby(['Pclass', 'Sex'])['Age'].transform('median'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3.5.2. The fare variable probably depends on the Class, since presumably Class 1 tickets were more expensive than Class 2 and 3. \n# Let's plot Fare agains Class to confirm this\n\nsns.violinplot(testSet, y = 'Fare', x = 'Pclass')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like passengers in Class 1 paid much more than passengers in Class 2 and 3. ","metadata":{}},{"cell_type":"code","source":"# 3.5.3. Let's replace the missing values in the Fare variable by the median fare of each passenger's Class group\n\ntestSet['Fare'] = testSet['Fare'].fillna(testSet.groupby(['Pclass'])['Fare'].transform('median'))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:33:15.213929Z","iopub.execute_input":"2024-04-25T14:33:15.214255Z","iopub.status.idle":"2024-04-25T14:33:15.220956Z","shell.execute_reply.started":"2024-04-25T14:33:15.214234Z","shell.execute_reply":"2024-04-25T14:33:15.219977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3.6. Since some of the variables are categorical, but we want to trian a logistic regression model, which needs numerical input,\n# let's transform all categorical variables (Sex and Embarked) into numerical ones in both the train and test datasets\n\ntrainSet['Sex'] = trainSet['Sex'].replace(['male', 'female'],[0, 1])\ntrainSet['Embarked'] = trainSet['Embarked'].replace(['S', 'C', 'Q'],[0, 1, 2])\n\ntestSet['Sex'] = testSet['Sex'].replace(['male', 'female'],[0, 1])\ntestSet['Embarked'] = testSet['Embarked'].replace(['S', 'C', 'Q'],[0, 1, 2])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:33:22.093357Z","iopub.execute_input":"2024-04-25T14:33:22.093721Z","iopub.status.idle":"2024-04-25T14:33:22.107611Z","shell.execute_reply.started":"2024-04-25T14:33:22.093693Z","shell.execute_reply":"2024-04-25T14:33:22.106501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**STEP 4: TRAIN THE LOGISTIC REGRESSION MODEL**\n\nWe'll train the model only using the following variables: \n* Pclass\n* Sex\n* Age \n* Fare, \n* Embarked, \n* SibSp (if they traveled with a sibling or spouse) \n* Parch (if they traveled with a child or parent)","metadata":{}},{"cell_type":"code","source":"# 4.1. First let's see how a simple logistic regression model does. Let's use all the variables plotted for now. \n# Let's fit the model\nmodel = LogisticRegression(max_iter = 200)\nmodel.fit(trainSet.loc[:,['Pclass','Sex', 'Age','Fare','Embarked', 'SibSp', 'Parch']], trainSet['Survived'])\n\n# then let's get the model's accuracy\npredY = model.predict(testSet.loc[:,['Pclass','Sex', 'Age','Fare','Embarked', 'SibSp', 'Parch']])\n\n# save prediction\noutcome = pd.DataFrame({'PassengerId': testSet['PassengerId'].values,'Survived': predY})\noutcome.to_csv('outcome.csv', index=False)  \nprint(outcome)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:34:33.839323Z","iopub.execute_input":"2024-04-25T14:34:33.839665Z","iopub.status.idle":"2024-04-25T14:34:33.876597Z","shell.execute_reply.started":"2024-04-25T14:34:33.839642Z","shell.execute_reply":"2024-04-25T14:34:33.875401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This approach resulted in a prediction accuracy of **0.76076**","metadata":{}},{"cell_type":"markdown","source":"To improve the performance of the model, and help reach convergence faster, in less than 100 iterations, let's scale the data. ","metadata":{}},{"cell_type":"code","source":"# 4.2.1. We could use the minmaxscaler function, which sets the minimum to 0 and the maximum to 1. However, as we saw that we have some outliers, \n# for instance in the Fare variable, we will use the standard scaler which sets mean to 0 and std to 1. \n\nstd_scaler = StandardScaler() # initialize standard scaler\n \ndf_scaled = std_scaler.fit_transform(trainSet.loc[:,['Pclass','Sex', 'Age','Fare','Embarked', 'SibSp', 'Parch']].to_numpy())\ntrainScaled = pd.DataFrame(df_scaled, columns=['Pclass','Sex', 'Age','Fare','Embarked', 'SibSp', 'Parch'])# scale training dataset\n\ndf_scaled = std_scaler.fit_transform(testSet.loc[:,['Pclass','Sex', 'Age','Fare','Embarked', 'SibSp', 'Parch']].to_numpy())\ntestScaled = pd.DataFrame(df_scaled, columns=['Pclass','Sex', 'Age','Fare','Embarked', 'SibSp', 'Parch'])# scale test dataset\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4.2.2. Let's train the model and test its accuracy with the scaled variables\n\nmodel = LogisticRegression(max_iter = 100) # initialize model\nmodel.fit(trainScaled, trainSet['Survived']) # train model\npredY = model.predict(testScaled) # make prediction\n\n# save prediction\noutcome = pd.DataFrame({'PassengerId': testSet['PassengerId'].values,'Survived': predY})\noutcome.to_csv('outcome2.csv', index=False)  \nprint(outcome)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scaling the data resulted in slight accuracy increase to: **0.76555**","metadata":{}},{"cell_type":"code","source":"# 4.3. Finally, it's time for some HYPERPARAMETER OPTIMIZATION. Let's find the hyperparameters of the model that result in best\n# model performance. In other words, the ones that have the best bias-variance tradeoff.\n\n# Initialize model\nmodel = LogisticRegression()\n\n# Define evaluation\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n\n# Set hyperparameters grid\nparam_grid = {\n    'penalty': ['l1', 'l2'],      # Regularization term\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n    'solver': ['liblinear', 'saga']      # Optimization algorithm\n}\n\n# Define search\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='accuracy', verbose=1) \n\n# Train model and iterate over hyperparameters\ngrid_search.fit(trainScaled, trainSet['Survived'])\n\n# Extract the best model and its parameters\nbest_params = grid_search.best_params_\nbest_model = grid_search.best_estimator_\nprint(\"Best Hyperparameters:\", best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best hyperparameters were: C = 0.01, penalty = L2, solver = saga","metadata":{}},{"cell_type":"code","source":"# Use the best model to predict\npredY = best_model.predict(testScaled)\n\n# Save prediction\noutcome = pd.DataFrame({'PassengerId': testSet['PassengerId'].values,'Survived': predY})\noutcome.to_csv('outcome3.csv', index=False)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After **hyperparameter optimization**, the accuracy increased to **0.77511**","metadata":{}}]}